{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf65594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "import json\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\" \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "\n",
    "from pprint import pprint as pprint\n",
    "from typing import List, Optional\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf075789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3288eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d85be909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "#from nltk.corpus import wordnet31 as wn31\n",
    "from wiktionaryparser import WiktionaryParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22788af1-499f-4b13-ae01-75d8637c8803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 01:47:03.997352: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from T5_WSD import T5_WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348a5f61-ca8c-4e4e-ba7c-942e81b35f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_MODEL = \"ViT-B/32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77836e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_model, preprocess = clip.load(CLIP_MODEL, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8ba511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(path, preprocessor):\n",
    "    img_files = os.listdir(path)\n",
    "    \n",
    "    imgs = {}\n",
    "    for file in tqdm(img_files):\n",
    "        file_path = os.path.join(path, file)\n",
    "        #img = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "        img = preprocess(Image.open(file_path)).unsqueeze(0)\n",
    "        imgs[file] = img\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf578e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = \"Dataset/VWSD/trial/all_images\"\n",
    "image_path = \"Dataset/VWSD/train/train_v1/train_images_v1\"\n",
    "\n",
    "image_dict_path = 'Temp/img_dict.pkl'\n",
    "if os.path.isfile(image_dict_path):\n",
    "    img_dict = pickle.load(open(image_dict_path,'rb'))\n",
    "else:\n",
    "    img_dict = image_loader(image_path,preprocess)\n",
    "    pickle.dump(img_dict, open(image_dict_path,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677bc8a-d28c-45d8-a772-0518bf338a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd02377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file_path = \"Dataset/VWSD/trial/trial.data.txt\"\n",
    "# gold_file_path = \"Dataset/VWSD/trial/trial.gold.txt\"\n",
    "\n",
    "data_file_path = \"Dataset/VWSD/train/train_v1/train.data.v1.txt\"\n",
    "gold_file_path = \"Dataset/VWSD/train/train_v1/train.gold.v1.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e3de865-8b29-4f9a-8cac-b55da8f0dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_def_path = 'Temp/GPT_Definition/GPT_Context_Definitions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bac1a96e-633e-49f2-935e-89a76893c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_definitions(object):\n",
    "    def __init__(self, GPT_def_path):\n",
    "        temp_dict = json.load(open(GPT_def_path))\n",
    "        \n",
    "        GPT_dict = {}\n",
    "        for key in temp_dict.keys():\n",
    "            for k in temp_dict[key]:\n",
    "                 GPT_dict[k] = []\n",
    "        for key in temp_dict.keys():\n",
    "            for k in temp_dict[key]:\n",
    "                 GPT_dict[k].append(temp_dict[key][k])\n",
    "        self.GPT_dict = GPT_dict\n",
    "        \n",
    "    def get_senses(self, target_word):\n",
    "        return self.GPT_dict[target_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8913fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary_wrapper(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #self.dictionary_type=dict_type\n",
    "        \n",
    "        self.wn = wn\n",
    "        self.wiktionary_parser = WiktionaryParser()\n",
    "        self.GPT_definitions = GPT_definitions(GPT_def_path)\n",
    "        \n",
    "    def get_wn_definitions(self, target_word):\n",
    "        sense_definitions = []\n",
    "        target_senses = self.wn.synsets(target_word)\n",
    "        for synset in target_senses:\n",
    "            #if synset.pos() == 'n':\n",
    "            sense_definition = synset.definition().split(';')[0]\n",
    "            sense_definitions.append(sense_definition)\n",
    "        sense_definitions = list(set(sense_definitions))\n",
    "        \n",
    "        return sense_definitions\n",
    "        \n",
    "    def get_wiktionary_definitions(self, target_word, lang):\n",
    "        parser = self.wiktionary_parser\n",
    "        sense_definitions = []\n",
    "        \n",
    "        target_senses = parser.fetch(target_word, lang)\n",
    "        #print(target_senses)\n",
    "        for synset in target_senses:\n",
    "            #print(synset)\n",
    "            for polysemy in synset['definitions']:\n",
    "                #print(definition)\n",
    "                for sense in polysemy['text'][1:]:\n",
    "                    sense_definition = sense.split(';')[0]\n",
    "                    #print(sense_definition)\n",
    "                sense_definitions.append(sense_definition)\n",
    "        sense_definitions = list(set(sense_definitions))\n",
    "        \n",
    "        return sense_definitions\n",
    "    \n",
    "    def get_GPT_definitions(self, target_word, lang):\n",
    "        return self.GPT_definitions.get_senses(target_word)\n",
    "    \n",
    "    def get_definitions(self, target_word, dictionary_type = \"wordnet\", lang='english'):\n",
    "        # dictionary: wordnet, wiktionary, both\n",
    "        #print(dictionary_type)\n",
    "        if dictionary_type == 'wordnet':\n",
    "            sense_definitions = self.get_wn_definitions(target_word)\n",
    "        elif dictionary_type == 'wiktionary':\n",
    "            sense_definitions = self.get_wiktionary_definitions(target_word, lang)\n",
    "        elif dictionary_type == 'GPT_gen':\n",
    "            sense_definitions = self.get_GPT_definitions(target_word, lang)\n",
    "        elif dictionary_type == 'both':\n",
    "            sense_definitions = self.get_wn_definitions(target_word)\n",
    "            sense_definitions += self.get_GPT_definitions(target_word, lang)\n",
    "        elif dictionary_type == 'compensate':\n",
    "            sense_definitions = self.get_wn_definitions(target_word)\n",
    "            if len(sense_definitions) == 0:\n",
    "                sense_definitions += self.get_GPT_definitions(target_word, lang)\n",
    "        return sense_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95bc791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_loader(data_file_path, gold_file_path = None):\n",
    "    \n",
    "#     text_data = {}\n",
    "    \n",
    "#     fin_data = open(data_file_path)\n",
    "#     for data_index, line in enumerate(fin_data):\n",
    "#         line = line.strip()\n",
    "#         if not line: continue\n",
    "        \n",
    "#         cols = line.split('\\t')\n",
    "#         target_word = cols[0]; context = cols[1]\n",
    "#         candidates = cols[2:]\n",
    "        \n",
    "#         sense_definitions = []\n",
    "#         target_senses = wn.synsets(target_word)\n",
    "#         for synset in target_senses:\n",
    "#             if synset.pos() == 'n':\n",
    "#                 sense_definition = synset.definition().split(';')[0]\n",
    "#                 sense_definitions.append(sense_definition)\n",
    "            \n",
    "#         text_data[data_index] = {'target_word': target_word,\n",
    "#                                  'sense_definitions': sense_definitions,\n",
    "#                                  'context': context,\n",
    "#                                  'candidates': candidates}\n",
    "#     fin_data.close()\n",
    "    \n",
    "    \n",
    "#     if gold_file_path:\n",
    "#         fin_gold = open(gold_file_path)\n",
    "#         for gold_index, line in enumerate(fin_gold):\n",
    "#             line = line.strip()\n",
    "#             if not line.strip(): continue\n",
    "            \n",
    "#             gold = line.strip()\n",
    "#             text_data[gold_index]['gold'] = gold\n",
    "            \n",
    "#     return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b285c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3efeeb0d-5306-4d80-8d84-bd1a3a3263c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd = None\n",
    "wsd = T5_WSD()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d2a98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_file_path, dictionary, dictionary_type=\"wordnet\", gold_file_path = None):\n",
    "    \n",
    "    def target_word_preprocessing(target_word):\n",
    "        #target_word = target_word.replace('-',' ')\n",
    "        return target_word\n",
    "        \n",
    "    \n",
    "    text_data = {}\n",
    "    \n",
    "    fin_data = open(data_file_path)\n",
    "    lines = fin_data.readlines()\n",
    "    candidate_lens = []\n",
    "    for data_index, line in tqdm(enumerate(lines)):\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        \n",
    "        cols = line.split('\\t')\n",
    "        target_word = cols[0]; target_word = target_word_preprocessing(target_word)\n",
    "        context = cols[1]\n",
    "        candidates = cols[2:]\n",
    "        \n",
    "        #sense_definitions = []\n",
    "        #target_senses = wn.synsets(target_word)\n",
    "        sense_definitions = dictionary.get_definitions(target_word, dictionary_type)\n",
    "        wordnet_definitions = dictionary.get_definitions(target_word, 'wordnet')\n",
    "\n",
    "\n",
    "#         for synset in target_senses:\n",
    "#             #if synset.pos() == 'n':\n",
    "#             sense_definition = synset.definition().split(';')[0]\n",
    "#             sense_definitions.append(sense_definition)\n",
    "#         sense_definitions = list(set(sense_definitions))\n",
    "        \n",
    "        answer_definition = []\n",
    "        #print(wordnet_definitions)\n",
    "        if wsd and len(wordnet_definitions) > 0:\n",
    "            if len(wordnet_definitions) > 1:\n",
    "                definition, index = wsd.predict(context, target_word, wordnet_definitions)\n",
    "                answer_definition = [wordnet_definitions[index]]\n",
    "            else:\n",
    "                answer_definition = wordnet_definitions\n",
    "        \n",
    "        text_data[data_index] = {'target_word': target_word,\n",
    "                                 'sense_definitions': sense_definitions,\n",
    "                                 'wordnet_definitions': wordnet_definitions,\n",
    "                                 'context': context,\n",
    "                                 'candidates': candidates,\n",
    "                                 'answer_definition': answer_definition}\n",
    "        # if len(candidates) != 10:\n",
    "        #     print(candidates); break\n",
    "        candidate_lens.append(len(candidates))\n",
    "    fin_data.close()\n",
    "    \n",
    "    \n",
    "    if gold_file_path:\n",
    "        fin_gold = open(gold_file_path)\n",
    "        for gold_index, line in enumerate(fin_gold):\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            \n",
    "            gold = line\n",
    "            text_data[gold_index]['gold'] = gold\n",
    "    print(np.mean(candidate_lens))\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3ebb997",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "519it [01:04, 14.91it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (850 > 512). Running this sequence through the model will result in indexing errors\n",
      "12869it [27:26,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_data = data_loader(data_file_path, \n",
    "                        dictionary,\n",
    "                        'wordnet',\n",
    "                        gold_file_path = gold_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4163ec8-8678-43c3-b7db-3588b17bb923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target_word': 'serinus',\n",
       " 'sense_definitions': ['Old World finches'],\n",
       " 'wordnet_definitions': ['Old World finches'],\n",
       " 'context': 'serinus genus',\n",
       " 'candidates': ['image.3.jpg',\n",
       "  'image.23.jpg',\n",
       "  'image.4.jpg',\n",
       "  'image.1.jpg',\n",
       "  'image.2.jpg',\n",
       "  'image.20.jpg',\n",
       "  'image.5.jpg',\n",
       "  'image.24.jpg',\n",
       "  'image.22.jpg',\n",
       "  'image.21.jpg'],\n",
       " 'answer_definition': ['Old World finches'],\n",
       " 'gold': 'image.20.jpg'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c6f3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_keys = list(text_data.keys())\n",
    "text_data_keys = text_data_keys\n",
    "text_data = {key: text_data[key] for key in text_data_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9d33ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4635200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CADG_analysis_PATH = 'Temp/GPT_Definition/CADG_analysis.txt'\n",
    "DG_analysis_PATH = 'Temp/GPT_Definition/DG_analysis.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7112bab7-e39d-477c-84ce-59faea8ae720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Def_Analysis(PATH):\n",
    "    fin = open(PATH)\n",
    "    Def_Analysis_Dict = {}\n",
    "    for line in fin:\n",
    "        context, target, definition, agreement  = line.strip().split('\\t')\n",
    "        Def_Analysis_Dict[context] = {'target': target,\n",
    "                                      'definition': definition,\n",
    "                                      'agreement': agreement}\n",
    "    return Def_Analysis_Dict\n",
    "CADG_analysis_dict = Def_Analysis(CADG_analysis_PATH)\n",
    "DG_analysis_dict = Def_Analysis(DG_analysis_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "63bc8d8d-782c-4f35-a2b0-fd7c0b1ff511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VWSD_CLIP_Zeroshot(object):\n",
    "    def __init__(self, CLIP_model, CLIP_preprocess):\n",
    "        self.CLIP_model = CLIP_model; \n",
    "        self.CLIP_preprocess = CLIP_preprocess\n",
    "    \n",
    "    def test(self, context, images):\n",
    "        CLIP_model = self.CLIP_model\n",
    "        CLIP_preprocess = self.CLIP_preprocess\n",
    "        \n",
    "        text = clip.tokenize([context]).to(device)\n",
    "        images = torch.stack(images).squeeze().to(device)\n",
    "        \n",
    "        image_features = CLIP_model.encode_image(images)\n",
    "        text_features = CLIP_model.encode_text(text)\n",
    "        \n",
    "        logits_per_image, logits_per_text = CLIP_model(images, text)\n",
    "        \n",
    "        \n",
    "    def evaluate_posterior(self, text_data, img_dict):\n",
    "        # I <- candidate images, T <- context, a <- ambiguous\n",
    "        # P(I|T) ~ P(I,T)/ <- CLIP\n",
    "        # 56.5% Accuracy 10% random\n",
    "        CLIP_model = self.CLIP_model\n",
    "\n",
    "        preds = []\n",
    "        golds = []\n",
    "        answers = []\n",
    "        partial_answers = []\n",
    "        data_indexes = []\n",
    "        for data_index in tqdm(text_data.keys()):\n",
    "            data = text_data[data_index]\n",
    "            context = data['context']; candidates = data['candidates']\n",
    "            \n",
    "            gold = data['gold']; gold_index = data['candidates'].index(gold)\n",
    "            data_indexes.append(data_index)\n",
    "            #if len(data['wordnet_definitions'])<2: continue\n",
    "            text = clip.tokenize([context]).to(device)\n",
    "            with torch.no_grad():\n",
    "                images = [img_dict[candidate] for candidate in candidates]\n",
    "                images = torch.stack(images).squeeze().to(device)\n",
    "                image_features = CLIP_model.encode_image(images)\n",
    "                text_features = CLIP_model.encode_text(text)\n",
    "\n",
    "                logits_per_image, logits_per_text = CLIP_model(images, text)\n",
    "                probs = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
    "                pred = np.argmax(probs[0])\n",
    "                \n",
    "                preds.append(data['candidates'][pred]) \n",
    "                golds.append(gold)\n",
    "                if pred == gold_index:\n",
    "                    answers.append(1)\n",
    "                else:\n",
    "                    answers.append(0)\n",
    "                \n",
    "                sorted_indexes = reversed(np.argsort(probs[0]))\n",
    "                \n",
    "                i = 1\n",
    "                #print(sorted_indexes)\n",
    "                for index in sorted_indexes:\n",
    "                    #print(index, gold_index)\n",
    "                    if index == gold_index:\n",
    "                        #partial_answers = 1/i\n",
    "                        partial_answers.append(1/i)\n",
    "                        break\n",
    "                    i+=1\n",
    "        return preds, golds, answers, partial_answers, data_indexes\n",
    "    \n",
    "    \n",
    "    def evaluate_bayesian_posterior(self, text_data, img_dict):\n",
    "        # P(I|T) -> \\sigma \\simga P(I|D,T)P(D|T)\n",
    "        # 75%\n",
    "        CLIP_model = self.CLIP_model\n",
    "\n",
    "        preds = []\n",
    "        golds = []\n",
    "        answers = []\n",
    "        partial_answers = []\n",
    "        data_indexes = []\n",
    "        #probs = []\n",
    "        for data_index in tqdm(text_data.keys()):\n",
    "            data = text_data[data_index]\n",
    "            context = data['context']; candidates = data['candidates']\n",
    "            sense_definitions = data['sense_definitions']\n",
    "            \n",
    "            \n",
    "            \n",
    "            #if len(data['wordnet_definitions'])<2: continue\n",
    "            #sense_definitions = data['answer_definition']\n",
    "            sense_definitions = [s for s in sense_definitions if s not in data['answer_definition']]\n",
    "            #print(context, data['target_word'], data['answer_definition'])\n",
    "            \n",
    "            \n",
    "            if context in CADG_analysis_dict:\n",
    "                if CADG_analysis_dict[context]['agreement'] == 'FALSE':\n",
    "                    sense_definitions = [CADG_analysis_dict[context]['definition']]\n",
    "                else: continue\n",
    "                #sense_definitions = []\n",
    "            else:\n",
    "                continue\n",
    "            #print()\n",
    "            sense_definitions = [context + ' : ' + sense_definition for sense_definition in sense_definitions]\n",
    "            if not len(sense_definitions):\n",
    "                #print('no sense')\n",
    "                sense_definitions += [context]\n",
    "            random.shuffle(sense_definitions)\n",
    "            sense_definitions = sense_definitions[0]\n",
    "            \n",
    "            gold = data['gold']; gold_index = data['candidates'].index(gold)\n",
    "            \n",
    "            data_indexes.append(data_index)\n",
    "\n",
    "            #text = clip.tokenize([context]).to(device)\n",
    "            with torch.no_grad():\n",
    "                context_text = clip.tokenize([context], truncate = True).to(device)\n",
    "                definition_text = clip.tokenize(sense_definitions, truncate = True).to(device)\n",
    "\n",
    "                images = [img_dict[candidate] for candidate in candidates]\n",
    "                images = torch.stack(images).squeeze().to(device)\n",
    "\n",
    "                # 1 answer and 9 distractors\n",
    "                image_features = CLIP_model.encode_image(images)\n",
    "                text_features = CLIP_model.encode_text(context_text)\n",
    "                # 4 senses in wordnet [4X512]\n",
    "                def_features = CLIP_model.encode_text(definition_text)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # probs text to def\n",
    "                # P(D_i|T)\n",
    "                # [1X4]\n",
    "                logits_per_definition = torch.matmul(text_features, def_features.T)\n",
    "                prob_dist_definitions = logits_per_definition.softmax(dim=-1)\n",
    "                \n",
    "                \n",
    "                # print(context)\n",
    "                # print(sense_definitions)\n",
    "                # print(logits_per_definition)\n",
    "                # print(prob_dist_definitions)\n",
    "                # P(I|T,D)\n",
    "                # [4 X 10] \n",
    "                logits_per_image, logits_per_text = CLIP_model(images, definition_text)\n",
    "                probs_per_image = logits_per_image.softmax(dim=-1)\n",
    "                probs_per_text = logits_per_text.softmax(dim=-1)\n",
    "\n",
    "                bayesian_probs = torch.matmul(prob_dist_definitions, probs_per_text).cpu().numpy()\n",
    "                pred = np.argmax(bayesian_probs)\n",
    "                \n",
    "                sorted_indexes = reversed(np.argsort(bayesian_probs[0]))\n",
    "                \n",
    "                i = 1\n",
    "                for index in sorted_indexes:\n",
    "                    if index == gold_index:\n",
    "                        #partial_answers = 1/i\n",
    "                        partial_answers.append(1/i)\n",
    "                        break\n",
    "                    i+=1\n",
    "                #ranks = [data['candidates'][index] for index in sorted_indexes]\n",
    "                \n",
    "                preds.append(data['candidates'][pred]) \n",
    "                golds.append(gold)\n",
    "                if pred == gold_index:\n",
    "                    answers.append(1)\n",
    "                else:\n",
    "                    answers.append(0)\n",
    "        return preds, golds, answers, partial_answers, data_indexes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ad6d32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VWSD_CLIP = VWSD_CLIP_Zeroshot(CLIP_model, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74a1fef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12869/12869 [11:23<00:00, 18.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.98\n",
      "MRR: 82.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p_preds, p_golds, p_answers, p_partial_answers, data_indexes =  VWSD_CLIP.evaluate_posterior(text_data, img_dict)\n",
    "print(\"Accuracy:\", \"%.2f\"%(np.mean(p_answers)*100))\n",
    "print(\"MRR:\", \"%.2f\"%(np.mean(p_partial_answers)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc365f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 425), (1, 2032), (2, 323), (3, 190), (4, 100), (5, 80), (6, 61), (7, 41), (8, 31), (9, 20), (10, 17), (11, 14), (12, 14), (13, 11), (14, 13), (15, 11), (16, 17), (17, 3), (18, 8), (19, 4), (20, 6), (21, 5), (22, 4), (23, 2), (24, 4), (25, 3), (27, 4), (28, 2), (29, 3), (30, 1), (31, 2), (32, 1), (33, 3), (34, 3), (36, 1), (37, 1), (39, 2), (40, 1), (44, 1), (45, 4), (49, 1), (51, 2), (52, 2), (54, 1), (70, 2), (75, 1)]\n",
      "[(0, 1420), (1, 5058), (2, 1270), (3, 539), (4, 299), (5, 190), (6, 144), (7, 113), (8, 75), (9, 51), (10, 26), (11, 36), (12, 22), (13, 18), (14, 19), (15, 14), (16, 13), (17, 7), (18, 18), (19, 6), (20, 5), (21, 7), (22, 2), (23, 5), (24, 2), (25, 3), (26, 2), (27, 3), (28, 1), (29, 2), (31, 2), (33, 2), (34, 4), (35, 1), (39, 2), (41, 2), (45, 3), (47, 2), (52, 1), (57, 2), (75, 1)]\n",
      "Hits@1 |D^t|==0: 76.96\n",
      "Hits@1 |D^t|==1: 71.34\n",
      "Hits@1 |D^t|>1: 74.07\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "p_sense_nums_w = []\n",
    "p_sense_nums_r = []\n",
    "for t, p, g in zip(data_indexes, p_preds, p_golds):\n",
    "    if p != g:\n",
    "        #print(t, text_data[t]['context'], p, g, len(text_data[t]['sense_definitions']))\n",
    "        p_sense_nums_w.append(len(text_data[t]['wordnet_definitions']))\n",
    "    else:\n",
    "        p_sense_nums_r.append(len(text_data[t]['wordnet_definitions']))\n",
    "    index+=1\n",
    "    \n",
    "print(sorted(Counter(p_sense_nums_w).items()))\n",
    "print(sorted(Counter(p_sense_nums_r).items()))\n",
    "\n",
    "right_when_zero = sorted(Counter(p_sense_nums_r).items())[0][1]\n",
    "wrong_when_zero = sorted(Counter(p_sense_nums_w).items())[0][1]\n",
    "\n",
    "right_when_one = sorted(Counter(p_sense_nums_r).items())[1][1]\n",
    "wrong_when_one = sorted(Counter(p_sense_nums_w).items())[1][1]\n",
    "\n",
    "right_when_over_one = 0\n",
    "wrong_when_over_one = 0\n",
    "\n",
    "for s, c in sorted(Counter(p_sense_nums_w).items()):\n",
    "    if s > 1: wrong_when_over_one += c\n",
    "for s, c in sorted(Counter(p_sense_nums_r).items()):\n",
    "    if s > 1: right_when_over_one += c\n",
    "\n",
    "print('Hits@1 |D^t|==0: %.2f'%(right_when_zero/(right_when_zero + wrong_when_zero)*100))\n",
    "print('Hits@1 |D^t|==1: %.2f'%(right_when_one/(right_when_one + wrong_when_one)*100))\n",
    "print('Hits@1 |D^t|>1: %.2f'%(right_when_over_one/(right_when_over_one + wrong_when_over_one)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00247246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data_indexes), len(bp_partial_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9e9d7c4c-3632-4068-a921-4ce1c9480e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['image.2247.jpg',\n",
       "  'image.7871.jpg',\n",
       "  'image.1840.jpg',\n",
       "  'image.5307.jpg',\n",
       "  'image.13627.jpg',\n",
       "  'image.1084.jpg',\n",
       "  'image.8488.jpg',\n",
       "  'image.14101.jpg',\n",
       "  'image.13716.jpg',\n",
       "  'image.3999.jpg',\n",
       "  'image.773.jpg',\n",
       "  'image.3193.jpg',\n",
       "  'image.5064.jpg',\n",
       "  'image.10030.jpg',\n",
       "  'image.4872.jpg',\n",
       "  'image.11985.jpg',\n",
       "  'image.4349.jpg',\n",
       "  'image.12937.jpg'],\n",
       " ['image.2247.jpg',\n",
       "  'image.7871.jpg',\n",
       "  'image.1840.jpg',\n",
       "  'image.5307.jpg',\n",
       "  'image.13627.jpg',\n",
       "  'image.1084.jpg',\n",
       "  'image.8488.jpg',\n",
       "  'image.14101.jpg',\n",
       "  'image.13716.jpg',\n",
       "  'image.3999.jpg',\n",
       "  'image.773.jpg',\n",
       "  'image.3193.jpg',\n",
       "  'image.4.jpg',\n",
       "  'image.10030.jpg',\n",
       "  'image.4872.jpg',\n",
       "  'image.11985.jpg',\n",
       "  'image.4349.jpg',\n",
       "  'image.12937.jpg'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_golds, bp_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2dcc7842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12869/12869 [00:01<00:00, 10595.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.89\n",
      "MRR: 91.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bp_preds, bp_golds, bp_answers, bp_partial_answers, data_indexes =  VWSD_CLIP.evaluate_bayesian_posterior(text_data, img_dict)\n",
    "print(\"Accuracy:\", \"%.2f\"%(np.mean(bp_answers)*100))\n",
    "print(\"MRR:\", \"%.2f\"%(np.mean(bp_partial_answers)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db9b6629-a32f-4e27-9d1d-6e3ae6737b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 281), (3, 199), (4, 103), (5, 75), (6, 62), (7, 44), (8, 30), (9, 19), (10, 15), (11, 23), (12, 11), (13, 9), (14, 13), (15, 11), (16, 21), (17, 5), (18, 13), (19, 5), (20, 4), (21, 5), (22, 3), (23, 2), (24, 5), (25, 4), (26, 1), (27, 3), (28, 2), (29, 4), (30, 1), (32, 1), (33, 2), (34, 4), (36, 1), (37, 1), (39, 4), (40, 1), (44, 1), (45, 3), (51, 2), (52, 1), (54, 1), (70, 2), (75, 1)]\n",
      "[(2, 1312), (3, 530), (4, 296), (5, 195), (6, 143), (7, 110), (8, 76), (9, 52), (10, 28), (11, 27), (12, 25), (13, 20), (14, 19), (15, 14), (16, 9), (17, 5), (18, 13), (19, 5), (20, 7), (21, 7), (22, 3), (23, 5), (24, 1), (25, 2), (26, 1), (27, 4), (28, 1), (29, 1), (31, 4), (33, 3), (34, 3), (35, 1), (41, 2), (45, 4), (47, 2), (49, 1), (52, 2), (57, 2), (75, 1)]\n",
      "Hits@1 |D^t|==0: 82.36\n",
      "Hits@1 |D^t|==1: 72.70\n",
      "Hits@1 |D^t|>1: 74.63\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "pb_sense_nums_w = []\n",
    "pb_sense_nums_r = []\n",
    "for t, p, g in zip(data_indexes, bp_preds, bp_golds):\n",
    "    if p != g:\n",
    "        #print(t, text_data[t]['context'], p, g, len(text_data[t]['sense_definitions']))\n",
    "        pb_sense_nums_w.append(len(text_data[t]['wordnet_definitions']))\n",
    "    else:\n",
    "        pb_sense_nums_r.append(len(text_data[t]['wordnet_definitions']))\n",
    "    index+=1\n",
    "print(sorted(Counter(pb_sense_nums_w).items()))\n",
    "print(sorted(Counter(pb_sense_nums_r).items()))\n",
    "\n",
    "right_when_zero = sorted(Counter(pb_sense_nums_r).items())[0][1]\n",
    "wrong_when_zero = sorted(Counter(pb_sense_nums_w).items())[0][1]\n",
    "\n",
    "right_when_one = sorted(Counter(pb_sense_nums_r).items())[1][1]\n",
    "wrong_when_one = sorted(Counter(pb_sense_nums_w).items())[1][1]\n",
    "\n",
    "right_when_over_one = 0\n",
    "wrong_when_over_one = 0\n",
    "\n",
    "for s, c in sorted(Counter(pb_sense_nums_w).items()):\n",
    "     if s > 1: wrong_when_over_one += c\n",
    "for s, c in sorted(Counter(pb_sense_nums_r).items()):\n",
    "     if s > 1: right_when_over_one += c\n",
    "    \n",
    "print('Hits@1 |D^t|==0: %.2f'%(right_when_zero/(right_when_zero + wrong_when_zero)*100))\n",
    "print('Hits@1 |D^t|==1: %.2f'%(right_when_one/(right_when_one + wrong_when_one)*100))\n",
    "print('Hits@1 |D^t|>1: %.2f'%(right_when_over_one/(right_when_over_one + wrong_when_over_one)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfbe044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 0\n",
    "# p_sense_nums_w = []\n",
    "# p_sense_nums_r = []\n",
    "# for t, p, g in zip(text_data, p_preds, p_golds):\n",
    "#     if p != g:\n",
    "#         #print(t, text_data[t]['context'], p, g, len(text_data[t]['sense_definitions']))\n",
    "#         p_sense_nums_w.append(len(text_data[t]['wordnet_definitions']))\n",
    "#     else:\n",
    "#         p_sense_nums_r.append(len(text_data[t]['wordnet_definitions']))\n",
    "#     index+=1\n",
    "    \n",
    "# print(sorted(Counter(p_sense_nums_w).items()))\n",
    "# print(sorted(Counter(p_sense_nums_r).items()))\n",
    "\n",
    "# right_when_zero = sorted(Counter(p_sense_nums_r).items())[0][1]\n",
    "# wrong_when_zero = sorted(Counter(p_sense_nums_w).items())[0][1]\n",
    "\n",
    "# right_when_one = sorted(Counter(p_sense_nums_r).items())[1][1]\n",
    "# wrong_when_one = sorted(Counter(p_sense_nums_w).items())[1][1]\n",
    "\n",
    "# right_when_over_one = 0\n",
    "# wrong_when_over_one = 0\n",
    "\n",
    "# for s, c in sorted(Counter(p_sense_nums_w).items()):\n",
    "#     if s > 1: wrong_when_over_one += c\n",
    "# for s, c in sorted(Counter(p_sense_nums_r).items()):\n",
    "#     if s > 1: right_when_over_one += c\n",
    "\n",
    "# print('Hits@1 |D^t|==0: %.2f'%(right_when_zero/(right_when_zero + wrong_when_zero)*100))\n",
    "# print('Hits@1 |D^t|==1: %.2f'%(right_when_one/(right_when_one + wrong_when_one)*100))\n",
    "# print('Hits@1 |D^t|>1: %.2f'%(right_when_over_one/(right_when_over_one + wrong_when_over_one)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe173644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379168d-41ad-4e85-99b2-1eb4c0c4ae68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef45484-7e9e-4058-9ded-a8b5e4fe1454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bcd10e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac1e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019a294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d4a680a-a88d-423b-8d5f-0aec805c1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts = []\n",
    "# for t in text_data:\n",
    "#     if len(text_data[t]['wordnet_definitions']) > 1 and len(text_data[t]['wordnet_definitions']) < 6:\n",
    "#         contexts.append(text_data[t]['context'])\n",
    "# random.shuffle(contexts)\n",
    "\n",
    "# fout = open('Temp/sampled_contexts_for_oracle_test.txt','w')\n",
    "# for context in contexts[:200]:\n",
    "#     fout.write(context+'\\n')\n",
    "# fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2ddc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers = []\n",
    "\n",
    "# for data_index in tqdm(list(text_data.keys())):\n",
    "#     data = text_data[data_index]\n",
    "#     context = data['context']; target_word = data['target_word']; \n",
    "#     candidates = data['candidates']\n",
    "    \n",
    "#     sense_definitions = data['sense_definitions']\n",
    "#     sense_definitions = [context + ' : ' + sense_definition for sense_definition in sense_definitions]\n",
    "#     gold = data['gold']; gold_index = data['candidates'].index(gold)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         context_text = clip.tokenize([context]).to(device)\n",
    "#         definition_text = clip.tokenize(sense_definitions).to(device)\n",
    "        \n",
    "#         images = [img_dict[candidate] for candidate in candidates]\n",
    "#         images = torch.stack(images).squeeze().to(device)\n",
    "        \n",
    "#         image_features = CLIP_model.encode_image(images)\n",
    "#         text_features = CLIP_model.encode_text(context_text)\n",
    "#         def_features = CLIP_model.encode_text(definition_text)\n",
    "        \n",
    "#         # probs text to def\n",
    "#         logits_per_definition = torch.matmul(text_features, def_features.T)\n",
    "#         prob_dist_definitions = logits_per_definition.softmax(dim=-1)\n",
    "        \n",
    "#         logits_per_image, logits_per_text = CLIP_model(images, definition_text)\n",
    "#         probs_per_image = logits_per_image.softmax(dim=-1)\n",
    "#         probs_per_text = logits_per_text.softmax(dim=-1)\n",
    "        \n",
    "#         bayesian_probs = torch.matmul(prob_dist_definitions, probs_per_text).cpu().numpy()\n",
    "#         max_index = np.argmax(bayesian_probs)\n",
    "        \n",
    "#         #print(max_index, gold_index)\n",
    "#         if max_index == gold_index:\n",
    "#             answers.append(1)\n",
    "#         else:\n",
    "#             answers.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcebcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63f6f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_features.shape, def_features.shape, logits_per_definition.shape, probs_per_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ba99d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac4db706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sense_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac083ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word, context, data['gold'], candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f47bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1c35c-e0be-42a8-923e-3af5232e34c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20559c59-9c39-4fec-b127-08d1886a0cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32bd8afd-c59c-4d67-bb16-6682063caf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'clip' from '/home/sunjae/anaconda3/envs/my_env/lib/python3.7/site-packages/clip/__init__.py'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4d775de-2c0b-4b61-9c5e-0637f785dd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'clip' from '/home/sunjae/anaconda3/envs/my_env/lib/python3.7/site-packages/clip/__init__.py'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
